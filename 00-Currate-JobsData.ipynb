{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currating data to initially train our model\n",
    "\n",
    "**Goal** \n",
    "for an intelligent job search application we would like to use internet scraped jobs data.  However; we first need to overcome the hurdle of jobs that are the same but have different job titles. \n",
    "\n",
    "**Background** \n",
    "when jobs are scraped off the internet, they have a job title that may be associated with other job titles.  For example, an administrative assitant can also be labeled as an office admin, admin, or assistant.  \n",
    "\n",
    "**Solution** \n",
    "therefore, we will 'categorize' the job titles into a smaller known job title set (administrative_assistant, apprentice, painter, security_guard, other).  We will then train our model against a new set of web scraped data and determine if our model is able to properly categorize (re-title) the jobs we are interested in.  Also, we needed to generate more jobs as the \n",
    "\n",
    "**Note:** this tutorial is part of a larger work effort to create an intelligent job search application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "from boto3 import session\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Pull AWS access keys from environment variables previously set \n",
    "# in Jupyter Hub\n",
    "# =============================================================\n",
    "key_id          = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key      = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "session         = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "\n",
    "s3_client       = boto3.client('s3',\n",
    "                  aws_access_key_id=key_id,\n",
    "                  aws_secret_access_key=secret_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Download jobs file from S3 and place it into a dataframe (df)\n",
    "#===============================================================\n",
    "bucket_name     = 'rhods-pilot'\n",
    "file_name       = 'job_offers_202106190048.csv'\n",
    "new_file_name   = 'new_job_offers.csv'\n",
    "local_dest_dir  = os.path.join(os.getcwd(), 'downloaded-folder')\n",
    "\n",
    "s3_client.download_file(bucket_name, file_name, new_file_name)\n",
    "\n",
    "#place file contents into a dataframe for processing\n",
    "df              = pd.read_csv(new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Examine what data columns and data is available by printing\n",
    "# out the first 5 rows of scraped job listings\n",
    "# =============================================================\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# read in dataset, extract out the following columns\n",
    "# title, company, location, description, employment_type\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "#***************** WORK WITH MAX ON MULTIPLE DATA SOURCES and REQD COLUMNS **************************************************\n",
    "#straight from web scraping\n",
    "#col_list        = [\"id\",\"title\", \"company\", \"location\", \"description\", \"employent_type\"]  #employent_type misspelled in curratedjobs.csv\n",
    "#df              = pd.read_csv(new_file_name, usecols=col_list) \n",
    "\n",
    "col_list        = [\"id\",\"searched_keywords\", \"title\", \"currated_title\", \"company\", \"location\", \"description\", \"employent_type\"]  #employent_type misspelled in curratedjobs.csv\n",
    "df              = pd.read_csv(\"dataset/curratedjobs2.csv\", usecols=col_list) \n",
    "\n",
    "\n",
    "\n",
    "#create file containing all jobs but with the above columns only\n",
    "#curratejobs     = df.to_csv('dataset/curratedjobs2.csv')\n",
    "curratejobs     = df.to_csv('dataset/model_training_jobs2.csv')\n",
    "\n",
    "#create file containing all jobs but with the above columns only\n",
    "\n",
    "\n",
    "#create file containing only unique titles to determine num of titles we are dealing with\n",
    "#unique_title    = df['title'].unique()\n",
    "#all_titles      = pd.Series(unique_title)\n",
    "\n",
    "#save titles to csv\n",
    "#all_titles.to_csv('dataset/unique_titles.csv')\n",
    "#print(all_titles[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# process file and add category, subcategory, title\n",
    "# categories and sub categories taken from CareerBuilder.com\n",
    "# =============================================================\n",
    "\n",
    "#created training file with 5 job types:  Adminstrative Assistant, Dentist, Systems Engineer, Supply Chain Manager, Structural Engineer\n",
    "#need more data therefore use markovfy to create additional data\n",
    "\n",
    "#read in responses, only pull out client response, categorized issue and car symptom\n",
    "\n",
    "df = pd.read_csv('dataset/model_training_jobs2.csv') \n",
    "df = df.fillna('')\n",
    "df['currated_jobtitle']  = df.iloc[:,4]                            #issue\n",
    "df['combined_jobtitles'] = df.iloc[:,1]+df.iloc[:,2]+df.iloc[:,3]   #respose\n",
    "subset = df.iloc[:,-3:]\n",
    "\n",
    "\n",
    "\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to perform pip install markovify in launcher\n",
    "\n",
    "import markovify\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Markovify is a simple, extensible Markov chain generator\n",
    "# Its primary use is for building Markov models of large corpora\n",
    "# of text and generating random sentences from that. \n",
    "# =============================================================\n",
    "\n",
    "#Function builds the model according to what job title (e.g. dentist, administrative assistant, systems engineer etc...) is given\n",
    "def train_markov_type(data, currated_jobtitle):\n",
    "    return markovify.Text(data[data[\"currated_jobtitle\"] == currated_jobtitle].combined_jobtitles, retain_original=False, state_size=2)\n",
    "\n",
    "#Function takes one of the 'issue' models and creates a randomly-generated sentence of length up to 200 characters.  Note only creates '1' sentence\n",
    "def make_sentence(model, length=1000):\n",
    "    return model.make_short_sentence(length, max_overlap_ratio = .7, max_overlap_total=15)\n",
    "\n",
    "#build models\n",
    "admin_model          = train_markov_type(subset, \"administrative_assistant\")\n",
    "apprentice_model     = train_markov_type(subset, \"apprentice\")\n",
    "painter_model        = train_markov_type(subset, \"painter\")\n",
    "security_model       = train_markov_type(subset, \"security_guard\")\n",
    "other_model          = train_markov_type(subset, \"other\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these models with relative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# combine models with relative weights\n",
    "# =============================================================\n",
    "\n",
    "import numpy\n",
    "\n",
    "def generate_cases(models, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [1] * len(models)\n",
    "    \n",
    "    choices = [] # Array of tuples of weight and models\n",
    "    \n",
    "    total_weight = float(sum(weights))\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        choices.append((float(sum(weights[0:i+1])) / total_weight, models[i]))\n",
    "    \n",
    "    # Return a tuple of model and category that are randomly selected by given weights.\n",
    "    def choose_model():\n",
    "        r = numpy.random.uniform()\n",
    "        for (model_weight, model) in choices:\n",
    "            if r <= model_weight:\n",
    "                return model\n",
    "        return choices[-1][1]\n",
    "\n",
    "    while True:\n",
    "        local_model = choose_model() \n",
    "        # local_model[0]) is the markovify model, local_model[1] is the category\n",
    "        yield make_sentence(local_model[0]), local_model[1]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new job descriptions & classify them as:  administrative_assistant, apprentice, painter, security\n",
    "guard & Other\n",
    "\n",
    "Store new job descriptions and job titles in file:  generated_jobs_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "generated_cases = generate_cases([(admin_model,'administrative_assistant'), \n",
    "                                  (apprentice_model,'apprentice'), \n",
    "                                  (painter_model,'painter'),\n",
    "                                  (security_model,'security_guard'),\n",
    "                                  (other_model,'other')], [28,7,7,7,7])\n",
    "\n",
    "# Tuples with sentence and category\n",
    "sentence_tuples = [next(generated_cases)  for i in range(2000)]  # create 2000 sentence/category tuples\n",
    "\n",
    "# Write to csv file old one is testdata1.csv\n",
    "with open('dataset/generated_jobs_data.csv', 'w') as file:\n",
    "    writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "    writer.writerows(sentence_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have created a new data set.  There is however a problem we must overcome.  Machine Learning models cannot understand 'text'.  Therefore we must convert the textual data into some numeric form.\n",
    "\n",
    "We can do this, using Tokenization.  Jump to 02-TokenDemo.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
