{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3333f355-e624-4d66-a242-fae1659e05a1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa748f82-e89d-4353-bf85-744ce5dae28c",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to **characterize job titles** that have been scraped from the internet. In a job search, the job will have an alpha numeric id and suggested job title.  However not all jobs that are the same, will have the same title.  For example, an administrative assitant can also be labelled as an office administrator or admin.  For our tutorial, we will categorize any job titles that have these words as 'administrative assistant'.   This will be accomplished through the use of **Natural Language Processing (NLP)**.\n",
    "\n",
    "This will allow you to discover, step by step, how you can create the code doing the job title text processing.  In the last part of the workshop, this code will be **packaged to create a service** that you can query from an application.\n",
    "\n",
    "We will be training the model.  Once our model is trained, we can test the model by entering a web scraped job id (e.g. admin assistant office administrator administrative support at coreix 2597597309) and check if the model has correctly characterized the jot title.  Current job titles we are categorizing are:  administrative_assistant, apprentice, painter, security_guard or other.\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadbffe-164d-40e7-a66e-36c5e0cd4718",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "First, we'll need to **install some libraries** that are not part of our container image. Normally, **Red Hat OpenShift Data Science** is already taking care of this for you, based on what it detects in the code. **Red Hat OpenShift Data Science** will reinstall all those libraries for you every time you launch the notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4352-a1a1-4c9e-b1f3-7658186c591b",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Of course, we'll need to import various packages. They are either built in the notebook image you are running, or have been installed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee87a88b-e1bc-4bf9-b756-818c8bb69e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 13:43:10.777599: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6f8e-6f58-4d47-b818-c6983e9e73fc",
   "metadata": {},
   "source": [
    "## Create Training and Testing data sets\n",
    "\n",
    "Now that we have loaded the tools we need, the first step in our journey is to be able to take our raw data and divide it into testing and training sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693d83e0-8968-4ad8-9fea-56d881637b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "#Determine what the training and testing percentages, of the data set, will be.\n",
    "#============================================================================\n",
    "training_portion = .80         # Use 80% of data for training, 20% for testing\n",
    "max_words        = 1000        # Max words in job title input\n",
    "\n",
    "data             = pd.read_csv('dataset/generated_jobs_data.csv')\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and print out first 5 rows of \n",
    "# generated claims so you can see what data looks like.\n",
    "#\n",
    "# print(data.head())\n",
    "#============================================================================\n",
    "\n",
    "train_size       = int(len(data) * training_portion)\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  train_test_split\n",
    "# This function splits the data into training and test sets.  \n",
    "# Inputs:   raw data and determined train_size.\n",
    "#============================================================================\n",
    "def train_test_split(data, train_size):\n",
    "    train        = data[:train_size]\n",
    "    test         = data[train_size:]\n",
    "    return train, test\n",
    "\n",
    "train_cat, test_cat   = train_test_split(data.iloc[:,1], train_size)  # label data is second column\n",
    "train_text, test_text = train_test_split(data.iloc[:,0], train_size)  # text data is first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0af16-5205-4479-84f8-8dce26e328b0",
   "metadata": {},
   "source": [
    "## Tokenize the Data sets\n",
    "\n",
    "After we have training and testing sets, we need to **tokenize the data**.  This means that we convert text documents into contextual vectors which contain numeric representations (index of where those words occur in a word dictionary) of the words in the documents.\n",
    "\n",
    "To see how Tokenization works, you can take a look at **02-TokenDemo.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d01f6a-4e86-4ce6-bc37-b31b7747a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize              = Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data\n",
    "\n",
    "#============================================================================\n",
    "#x_train and x_test are the vectorization of the text data (which is a claim)\n",
    "#============================================================================\n",
    "x_train               = tokenize.texts_to_matrix(train_text)\n",
    "x_test                = tokenize.texts_to_matrix(test_text)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement and observe the rows in the \n",
    "# newly created matrix.\n",
    "#\n",
    "# print(x_train)\n",
    "# ===========================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4dcc9-c760-465d-9d18-bd274f66c7d3",
   "metadata": {},
   "source": [
    "## Using Sklearn\n",
    "We will be using the sklearn utility to convert label strings to numbered index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a64ce63-9c28-410d-b1c9-251a61b0c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Convert label strings to numbered index\n",
    "#============================================================================\n",
    "encoder              = LabelEncoder()  \n",
    "encoder.fit(train_cat)\n",
    "y_train              = encoder.transform(train_cat)\n",
    "y_test               = encoder.transform(test_cat)\n",
    "\n",
    "#============================================================================\n",
    "# Note: for each row in the data, each entry represents the value of the label\n",
    "# Example:  [2 1 1 2 1 1 0 ...  which corresponds to starter, other,\n",
    "# other, starter, other, other, brakes ...\n",
    "#\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statement.  What would you expect y_train\n",
    "# to look like?  \n",
    "#\n",
    "# print(y_train)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5be063-fafb-4c3a-b078-3578fec4c483",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "We need to create labels (job titles such as administrative assistant or painter) for our test data, convert the labels to numbered index and then use one-hot encoding.\n",
    "\n",
    "**One hot encoding** allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. **The categories must be converted into numbers**. This is required for both input and output variables that are categorical.\n",
    "\n",
    "After we have converted the labels using one-hot encoding, we will be ready to build our main NLP model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26517820-122d-41e3-97f1-ef3b4a09289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# Convert the labels to a one-hot representation\n",
    "#\n",
    "# One Hot Encoding replaces the column of labels whose (values are 0 or 1 or 2)\n",
    "# with 3 columns each representing 1 label value.  For example, the label \n",
    "# 'administrative_assitant' may be replaced by the vector 0 1 0, the label 'painter' may be replaced by\n",
    "# the vector 0 0 1, the label 'other' maybe replaced by the vector 1 0 0\n",
    "#============================================================================\n",
    "num_classes          = len(set(y_train))  # set() creates a unique set of objects\n",
    "y_train              = to_categorical(y_train, num_classes)  \n",
    "y_test               = to_categorical(y_test, num_classes)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements in order to inspect the \n",
    "# dimenstions of our training and test data.\n",
    "# y_train may appear as y_train shape: (159, 3) which represents 159 rows, 3 cols\n",
    "#\n",
    "#print('x_train shape:', x_train.shape)\n",
    "#print('x_test shape:', x_test.shape)\n",
    "#print('y_train shape:', y_train.shape)\n",
    "#print('y_test shape:', y_test.shape)\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248edb3-1339-4a33-ba14-bf6222334348",
   "metadata": {},
   "source": [
    "\n",
    "## Building the model\n",
    "\n",
    "Once the model is trained, we can test our model by entering a job title (and id) and check if the model has correctly categorized the job title. \n",
    "\n",
    "For example:  \n",
    "\n",
    "**admin assistant office administrator administrative support at coreix 2597597309**  is categorized as an **administrative_assistant**\n",
    "\n",
    "**security officer at unitrust protection services uk ltd 2589892274** is categorized as a **security_guard**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bce1a11-aab6-4d1f-9957-d3a06c7b513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 13:43:32.640783: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-12 13:43:32.641068: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-08-12 13:43:32.641093: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-12 13:43:32.641131: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterhub-nb-adrezni): /proc/driver/nvidia/version does not exist\n",
      "2021-08-12 13:43:32.641332: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-12 13:43:32.645661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-12 13:43:32.717930: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-12 13:43:32.718465: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "45/45 [==============================] - 1s 14ms/step - loss: 1.1554 - accuracy: 0.6088 - val_loss: 0.2552 - val_accuracy: 0.9750\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1728 - accuracy: 0.9957 - val_loss: 0.0495 - val_accuracy: 0.9937\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# Build model\n",
    "#============================================================================\n",
    "layers               = keras.layers\n",
    "models               = keras.models\n",
    "model                = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(max_words,), activation='relu'))  # Hidden layer with 512 nodes\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#============================================================================\n",
    "# relu, softmax, categorical_crossentropy are telling the model how to do some \n",
    "# internal calculations.  Softmax is telling the model to calculate \n",
    "# probabilities for each category in each document.  If you only had yes, \n",
    "# or no you would use sigmoid instead of softmax.\n",
    "#============================================================================\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#============================================================================\n",
    "# VARIABLES\n",
    "# history    - normally used to plot learning curves.  \n",
    "# fit        - calculates the weights in the model. \n",
    "# batch_size - tells the internal calculations how many rows to process at 1 time\n",
    "# epochs     - num of times model calculations will pass through the entire data\n",
    "#============================================================================\n",
    "batch_size          = 32\n",
    "epochs              = 2\n",
    "history = model.fit(x_train, y_train,      \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=epochs,         \n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "#============================================================================\n",
    "# evaluate func compares the model predictions with the actual known test values\n",
    "#============================================================================\n",
    "score = model.evaluate(x_test, y_test,       \n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "#============================================================================\n",
    "# TRY:  uncomment the below print statements to see the test loss and accuracy\n",
    "# of our model\n",
    "#\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "#============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5863ab-e446-4b23-9f87-37e73f546425",
   "metadata": {},
   "source": [
    "\n",
    "## Let's test our model!\n",
    "\n",
    "Now that we have a model, we would like to generate a prediction (e.g. categorize the web scraped job title as:  administrative_assitant, apprentice, painter, security_guard or other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157b9c35-7cf9-4302-b489-64fb2df23e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the job desc: admin assistant office administrator administrative support at coreix 2597597309 \n",
      "job category is: administrative_assistant\n"
     ]
    }
   ],
   "source": [
    "#============================================================================\n",
    "# In this section, we will need to create a 'predict' function that will pass \n",
    "# a 'job title string' to the model.  The return string will be a category of \n",
    "# administrative_assitant, apprentice, painter, security_guard or other.\n",
    "#============================================================================\n",
    "text_labels = encoder.classes_   #ndarray of output values (labels or classes)  e.g. security_guard\n",
    "#print(text_labels)\n",
    "\n",
    "#==========================================================\n",
    "# Save labels (categories) to be used later when we run the\n",
    "# model in actual flask app\n",
    "#==========================================================\n",
    "import csv\n",
    "\n",
    "with open('dataset/savedCategories.csv', 'w') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerow(text_labels)\n",
    "    \n",
    "\n",
    "#============================================================================\n",
    "# Examine first 10 test samples of 445\n",
    "#============================================================================\n",
    "#for i in range(len(test_cat)):\n",
    "    #temp = x_test[i]\n",
    "    #prediction = model.predict(np.array([x_test[i]]))\n",
    "    #predicted_label = text_labels[np.argmax(prediction)]  #predicted class\n",
    "    #print(test_text.iloc[i][:50], \"...\")                # 50 char sample of text\n",
    "    #print('Actual label:' + test_cat.iloc[i])\n",
    "    #print(\"Predicted label: \" + predicted_label + \"\\n\")\n",
    "\n",
    "#============================================================================\n",
    "# FUNCTION:  predict\n",
    "# This function takes a string input (e.g. web scraped job title and id), tokenizes the input\n",
    "# and then passes the input to the model.  The 'predicted_label' maps the\n",
    "# index of the prediction to the test labels array (e.g. administrative_assitant, apprentice, painter, security_guard or other)\n",
    "# The single_predicted_label returns one of the test labels array (e.g. painter)\n",
    "#============================================================================\n",
    "def predict(single_test_text):\n",
    "    text_as_series = pd.Series(single_test_text) #perform data conversion\n",
    "    single_x_test = tokenize.texts_to_matrix(text_as_series)\n",
    "    single_prediction = model.predict(np.array([single_x_test]))\n",
    "    model.save('models/jobmodel.h5')  #after prediction, save the model\n",
    "    single_predicted_label = text_labels[np.argmax(single_prediction)]  #maps index of the prediction to the test labels array e.g. painter\n",
    "    return (single_predicted_label)\n",
    "\n",
    "#========================================\n",
    "#Run the first time in order to save the model\n",
    "#=========================================\n",
    "single_test_text = 'admin assistant office administrator administrative support at coreix 2597597309 ' \n",
    "print('the job desc: ' + single_test_text)    #print out the job title and id being categorized\n",
    "\n",
    "prediction = predict(single_test_text) \n",
    "print('job category is: ' + prediction)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e603b3-f4e2-428f-857b-8829d4ba270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the job title: security officer at unitrust protection services uk ltd 2589892274\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1000), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\"), but it was called on an input with incompatible shape (None, 1, 1000).\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f73682e61f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "job category is: security_guard\n"
     ]
    }
   ],
   "source": [
    "#==========================================================================\n",
    "# TRY: uncomment the below to test the predict function.  We will test for 5\n",
    "# job titles so that we can see if the model can properly categorize job titles \n",
    "# that have been scraped from the internet\n",
    "# administrative assistant, apprentice, painter, security guard, other\n",
    "\n",
    "#single_test_text = 'security officer'\n",
    "#single_test_text = 'security guard' \n",
    "#single_test_text = 'security guard 6 month fixed term at g4s 2585133095'\n",
    "#single_tezt_text = 'admin assistant office administrator administrative support at coreix 2597597309'\n",
    "single_test_text = 'security officer at unitrust protection services uk ltd 2589892274'\n",
    "\n",
    "print('the job title: ' + single_test_text)               #print out the repair being categorized\n",
    "model = keras.models.load_model('models/jobmodel.h5')     #load the model before called predict function\n",
    "prediction = predict(single_test_text)                    #make the prediction\n",
    "print('job category is: ' + prediction)                   #print the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ea007-f9b0-4a20-9cb8-9854a6f8df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
